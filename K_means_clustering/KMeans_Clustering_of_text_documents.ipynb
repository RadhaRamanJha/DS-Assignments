{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Reference :-\n",
        "https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"
      ],
      "metadata": {
        "id": "rORKCQej3AMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading The the Text Data"
      ],
      "metadata": {
        "id": "GAUlZyNj2NSV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfjbZ0tdeHXo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "categories = [\n",
        "    'alt.atheism',\n",
        "    'comp.graphics',\n",
        "    'comp.sys.ibm.pc.hardware',\n",
        "    'comp.sys.mac.hardware',\n",
        "    'misc.forsale',\n",
        "    'rec.autos',\n",
        "    'rec.motorcycles',\n",
        "    'sci.electronics',\n",
        "    'talk.politics.guns',\n",
        "    'talk.politics.mideast',\n",
        "    'talk.religion.misc',\n",
        "]"
      ],
      "metadata": {
        "id": "8MszHuw6elg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = fetch_20newsgroups(\n",
        "    remove = ('headers','footers','quotes'),\n",
        "    subset = 'all',\n",
        "    categories = categories,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "labels = datasets.target\n",
        "unique_labels, category_size = np.unique(labels, return_counts=True)\n",
        "true_k = unique_labels.shape[0]\n",
        "print(f\"There are {len(datasets.data)} documents in {true_k} categories\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXJdfUQjruK4",
        "outputId": "0b510115-4ea8-40a8-e8d6-7903b2d2ba9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 10140 documents in 11 categories\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantifying the quality of clustering results."
      ],
      "metadata": {
        "id": "ej-opOzm2bne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eventhough Clustering algorithms are unsupervised learning methods. Since we have class labels for this specific dataset, it is possible to use evaluation metrics that leverage this “supervised” ground truth information to quantify the quality of the resulting clusters. Examples of such metrics are the following:\n",
        "\n",
        "1. Homogeneity, which quantifies how much clusters contain only members of a single class;\n",
        "\n",
        "2. Completeness, which quantifies how much members of a given class are assigned to the same clusters;\n",
        "\n",
        "3. V-measure, the harmonic mean of completeness and homogeneity;\n",
        "\n",
        "4. Rand-Index, which measures how frequently pairs of data points are grouped consistently according to the result of the clustering algorithm and the ground truth class assignment;\n",
        "\n",
        "5. Adjusted Rand-Index, a chance-adjusted Rand-Index such that random cluster assignment have an ARI of 0.0 in expectation."
      ],
      "metadata": {
        "id": "Mq7fsJju27ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from time import time\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "evaluations = []        # Empty list to store estimator name and training time\n",
        "evaluations_std = []    # Empty list to store estimator name and std dev of training time\n",
        "\n",
        "\n",
        "def fit_and_evaluate(km, X, name=None, n_runs=5):\n",
        "    name = km.__class__.__name__ if name is None else name\n",
        "\n",
        "    train_times = []\n",
        "    scores = defaultdict(list)\n",
        "    for seed in range(n_runs):\n",
        "        km.set_params(random_state=seed)\n",
        "        t0 = time()\n",
        "        km.fit(X)\n",
        "        train_times.append(time() - t0)\n",
        "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
        "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
        "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
        "        scores[\"Adjusted Rand-Index\"].append(\n",
        "            metrics.adjusted_rand_score(labels, km.labels_)\n",
        "        )\n",
        "        scores[\"Silhouette Coefficient\"].append(\n",
        "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
        "        )\n",
        "    train_times = np.asarray(train_times)\n",
        "\n",
        "    print(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
        "    evaluation = {\n",
        "        \"estimator\": name,\n",
        "        \"train_time\": train_times.mean(),\n",
        "    }\n",
        "    evaluation_std = {\n",
        "        \"estimator\": name,\n",
        "        \"train_time\": train_times.std(),\n",
        "    }\n",
        "    for score_name, score_values in scores.items():\n",
        "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
        "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
        "        evaluation[score_name] = mean_score\n",
        "        evaluation_std[score_name] = std_score\n",
        "    evaluations.append(evaluation)\n",
        "    evaluations_std.append(evaluation_std)"
      ],
      "metadata": {
        "id": "rpVz5CLG2XsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p4Wva_clRqu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction using TfidfVectorizer¶"
      ],
      "metadata": {
        "id": "1OivCbFRPCN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_df=0.5,\n",
        "    min_df=5,\n",
        "    stop_words=\"english\",\n",
        ")\n",
        "t0 = time()\n",
        "X_tfidf = vectorizer.fit_transform(datasets.data)\n",
        "\n",
        "print(f\"vectorization done in {time() - t0:.3f} s\")\n",
        "print(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K07VM-vPBTG",
        "outputId": "75a2cc96-f5b4-4954-d26a-a82fc59e2142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vectorization done in 5.268 s\n",
            "n_samples: 10140, n_features: 14779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After ignoring terms that appear in more than 50% of the documents (as set by max_df=0.5) and terms that are not present in at least 5 documents (set by min_df=5), the resulting number of unique terms n_features is around 14779. We can additionally quantify the sparsity of the X_tfidf matrix as the fraction of non-zero entries divided by the total number of elements."
      ],
      "metadata": {
        "id": "X-Wkh-hIQ8IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"There are {X_tfidf.nnz / np.prod(X_tfidf.shape):.3f}% entries in documents which are non-zero\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoPcCPh0Q7ym",
        "outputId": "bcff7b68-1e80-4a3f-dcb6-c61367259e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 0.003% entries in documents which are non-zero\n"
          ]
        }
      ]
    }
  ]
}